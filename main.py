from custom_tokens import custom_tokenizer, get_infrequent_tokens, mask_tokens
from indexing import encode
from model import Transformer
import pandas as pd
import torch, sys, random
import os

# –º–æ–π –≤—Å —Ç—É–ø–∏—Ç —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
sys.stdout.reconfigure(encoding='utf-8')

# –ó–∞–≥—Ä—É–∂–∞–µ–º Excel
df = pd.read_excel("result.xlsx")

# –ë–µ—Ä—ë–º –≤—Ç–æ—Ä–æ–π —Å—Ç–æ–ª–±–µ—Ü
texts = df["text"]

# –°–æ–µ–¥–∏–Ω—è–µ–º –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º <END>
corpus = f" <END> ".join(texts.astype(str))

# –°–ø–∏—Å–æ–∫ —Å–ø–µ—Ü—Ç–æ–∫–µ–Ω–æ–≤
spec_tokens = ["<END>", "<UNK>"]

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
tokens = custom_tokenizer(corpus, spec_tokens)

infreq_tokens = get_infrequent_tokens(tokens, min_count=2)
tokens = mask_tokens(tokens, infreq_tokens)


print(tokens[:10])  # –ø–µ—Ä–≤—ã–µ 10 —Ç–æ–∫–µ–Ω–æ–≤
print("–í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤:", len(tokens))
print("–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤:", len(set(tokens)))

# --- 3. –°–ª–æ–≤–∞—Ä—å ---
vocab_list = list(set(tokens))
word2idx = {w: i for i, w in enumerate(vocab_list)}
idx2word = {i: w for w, i in word2idx.items()}

# --- 4. –ö–æ–¥–∏—Ä–æ–≤–∫–∞ ---
enc = encode(tokens, vocab_list)  # (seq_len_total,)
print(enc)

# --- 5. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –±–∞—Ç—á–∏ ---
seq_len = 16
chunks = [enc[i:i+seq_len] for i in range(0, len(enc), seq_len)]
chunks = [c for c in chunks if len(c) == seq_len]
src = torch.stack(chunks)  # (batch, seq_len)

SOS_idx = word2idx["<END>"]  # —Ç–æ–∫–µ–Ω —Å—Ç–∞—Ä—Ç–∞
tgt = torch.cat([torch.full((src.size(0), 1), SOS_idx), src[:, :-1]], dim=1)

# --- 6. –ú–∞—Å–∫–∏ ---
src_mask = torch.ones(src.size(0), 1, src.size(1), dtype=torch.bool)
tgt_mask = torch.tril(torch.ones(tgt.size(1), tgt.size(1), dtype=torch.bool)).unsqueeze(0)

# --- 7. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
src_vocab = len(vocab_list)
tgt_vocab = len(vocab_list)

model = Transformer(src_vocab, tgt_vocab).to(device).to(device)
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

src, tgt = src.to(device), tgt.to(device)
src_mask, tgt_mask = src_mask.to(device), tgt_mask.to(device)

# --- –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è ---
checkpoint_path = "transformer_checkpoint.pth"

# --- –ó–∞–≥—Ä—É–∑–∫–∞, –µ—Å–ª–∏ –µ—Å—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ---
start_epoch = 1
if os.path.exists(checkpoint_path):
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint["model_state_dict"])
    optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
    start_epoch = checkpoint["epoch"] + 1
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: —ç–ø–æ—Ö–∞ {start_epoch - 1}")

# --- 8. –û–±—É—á–µ–Ω–∏–µ ---
epochs = 10
for epoch in range(start_epoch, epochs + 1):
    model.train()
    optimizer.zero_grad()
    out = model(src, tgt, src_mask, tgt_mask)  # (batch, seq_len, vocab)
    
    # üîπ –ë–´–õ–û: —Å—Ä–∞–≤–Ω–∏–≤–∞–ª–æ—Å—å —Å src
    # loss = criterion(out.view(-1, tgt_vocab), src.view(-1))
    # üîπ –°–¢–ê–õ–û: —Ç–µ–ø–µ—Ä—å —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç—Å—è —Å tgt (–ø—Ä–∞–≤–∏–ª—å–Ω–æ)
    loss = criterion(out.view(-1, tgt_vocab), tgt.view(-1))

    loss.backward()
    optimizer.step()

    print(f"Epoch {epoch}/{epochs} | Loss: {loss.item():.4f}")

    # --- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ---
    torch.save({
        "epoch": epoch,
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
        "loss": loss.item()
    }, checkpoint_path)
    print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {checkpoint_path}")


# --- 10. –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ---
def generate_from_enc(model, enc_tensor, idx2word, SOS_idx, max_len=10):
    model.eval()
    src = enc_tensor.unsqueeze(0).to(device)  # (1, seq_len)
    src_mask = torch.ones(1, 1, src.size(1), dtype=torch.bool, device=device)
    
    generated = torch.full((1, 1), SOS_idx, dtype=torch.long, device=device)
    for _ in range(max_len):
        tgt_mask = torch.tril(torch.ones(generated.size(1), generated.size(1), dtype=torch.bool, device=device)).unsqueeze(0)
        probs = model(src, generated, src_mask, tgt_mask)

        # üîπ –ë–´–õ–û: argmax (–∑–∞—Ü–∏–∫–ª–∏–≤–∞–ª–æ—Å—å)
        # next_token = torch.argmax(probs[:, -1, :], dim=-1, keepdim=True)

        # üîπ –°–¢–ê–õ–û: –¥–æ–±–∞–≤–∏–ª–∞ temperature + —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
        probs = torch.softmax(probs[:, -1, :] / 0.8, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)

        generated = torch.cat([generated, next_token], dim=1)
    
    tokens_out = [idx2word[int(i)] for i in generated[0, 1:]]  # –±–µ–∑ SOS
    return tokens_out


test_seq = chunks[random.randint(0, len(chunks)-1)]
gen_tokens = generate_from_enc(model, test_seq, idx2word, SOS_idx)
print("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:", " ".join(gen_tokens))


# --- 11. –§—É–Ω–∫—Ü–∏—è –æ–±—â–µ–Ω–∏—è —Å –º–æ–¥–µ–ª—å—é ---
def chat_with_model(model, word2idx, idx2word, SOS_idx, seq_len=16, max_gen_len=10):
    print("–ù–∞—á–∏–Ω–∞–µ–º —á–∞—Ç —Å –º–æ–¥–µ–ª—å—é! (–≤–≤–µ–¥–∏—Ç–µ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞)")
    model.eval()
    
    history_tokens = []  # –¥–ª—è –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    
    while True:
        user_input = input("–í—ã: ")
        if user_input.lower() == "exit":
            break
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –∑–∞–º–µ–Ω–∞ —Ä–µ–¥–∫–∏—Ö —Å–ª–æ–≤
        user_tokens = custom_tokenizer(user_input, spec_tokens)
        user_tokens = mask_tokens(user_tokens, infreq_tokens)
        
        history_tokens.extend(user_tokens)
        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ seq_len —Ç–æ–∫–µ–Ω–æ–≤
        context_tokens = history_tokens[-seq_len:]
        
        # –ö–æ–¥–∏—Ä–æ–≤–∫–∞
        context_enc = encode(context_tokens, vocab_list)
        context_enc = torch.tensor(context_enc, dtype=torch.long).to(device) if not isinstance(context_enc, torch.Tensor) else context_enc.detach().clone().long().to(device)

        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        gen_tokens = generate_from_enc(model, context_enc, idx2word, SOS_idx, max_len=max_gen_len)
        
        print("–ú–æ–¥–µ–ª—å:", " ".join(gen_tokens))
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è –¥–∏–∞–ª–æ–≥–∞
        history_tokens.extend(gen_tokens)


# --- 12. –ó–∞–ø—É—Å–∫ —á–∞—Ç–∞ ---
chat_with_model(model, word2idx, idx2word, SOS_idx)